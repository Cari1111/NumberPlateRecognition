{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch with cuda\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO(\"best.onnx\")\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "path = os.path.join(os.path.abspath(\".\"),\"License Plate Recognition.v6i.yolov11.zip\")\n",
    "\n",
    "# # Train the model\n",
    "# train_results = model.train(\n",
    "#     data=path,  # path to dataset YAML\n",
    "#     epochs=100,  # number of training epochs\n",
    "#     # workers=100,\n",
    "#     # patience=20,\n",
    "#     imgsz=640,  # training image size\n",
    "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
    "# )\n",
    "\n",
    "# # Evaluate model performance on the validation set\n",
    "# metrics = model.val()\n",
    "\n",
    "# # Export the model to ONNX format\n",
    "# path = model.export(format=\"onnx\")  # return path to exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 4 License_Plates, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 46.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 License_Plates, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 License_Plates, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 License_Plates, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from plate_recognition import NumberPlateRecognition\n",
    "\n",
    "rec = NumberPlateRecognition()\n",
    "\n",
    "def run(img):\n",
    "    rec.analyze(img)\n",
    "\n",
    "# Perform object detection on an image\n",
    "img = cv2.imread(\"example.jpg\")\n",
    "run(img)\n",
    "img = cv2.flip(img, 0)\n",
    "run(img)\n",
    "img = cv2.flip(img, 1)\n",
    "run(img)\n",
    "img = cv2.flip(img, 0)\n",
    "run(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = os.path.join(os.path.abspath(\".\"),\"models\",\"first10ktrain\",\"weights\",\"best.onnx\")\n",
    "model = YOLO(model_path, task='detect')\n",
    "\n",
    "def add_noise(bboxes, mean=0, std_dev=5):\n",
    "    noise = np.random.normal(mean, std_dev, bboxes.shape)\n",
    "    noisy_bboxes = bboxes + noise\n",
    "    return noisy_bboxes\n",
    "\n",
    "def extract_bboxes(results):\n",
    "    #bounding boxes can be extracted like that:\n",
    "    bboxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    return bboxes\n",
    "    #trying to add noise and add it on the picture (doesn't work yet)\n",
    "    # noisy_bboxes = add_noise(bboxes)\n",
    "    # results[0].boxes.xyxy = torch.tensor(noisy_bboxes, device=results[0].boxes.xyxy.device)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Perform object detection on an image\n",
    "img = cv2.imread(\"example.jpg\")\n",
    "results = model(img)\n",
    "\n",
    "# print(extract_bboxes(results))\n",
    "# print(results[0].boxes.cpu().numpy())\n",
    "# print(results[0])\n",
    "# results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading c:\\Ordner\\Uni\\Master\\24-Winter\\Bildverarbeitung\\NumberPlateRecognition\\models\\first10ktrain\\weights\\best.onnx for ONNX Runtime inference...\n",
      "Preferring ONNX Runtime TensorrtExecutionProvider\n",
      "*************** EP Error ***************\n",
      "EP Error D:\\a\\_work\\1\\s\\onnxruntime\\python\\onnxruntime_pybind_state.cc:507 onnxruntime::python::RegisterTensorRTPluginsAsCustomOps Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
      "****************************************\n",
      "\n",
      "0: 640x640 4 License_Plates, 10.0ms\n",
      "Speed: 79.0ms preprocess, 10.0ms inference, 155.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "boxes: [[     431.77      437.72      476.63      453.35]\n",
      " [     626.27      432.12      674.95      446.11]\n",
      " [        129      441.88      168.72      454.66]\n",
      " [     301.74      563.35      362.12      580.69]]\n",
      "conf: [    0.61744     0.59421     0.57782     0.46563]\n",
      "cls: [          0           0           0           0]\n",
      "clslgd: {0: 'License_Plate'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from safe_video import NumberPlateRecognition\n",
    "import cv2\n",
    "\n",
    "rec = NumberPlateRecognition()\n",
    "#img = cv2.imread(\"example.jpg\")\n",
    "img = cv2.imread(\"example.jpg\")\n",
    "img = img[:, :, ::-1]\n",
    "\n",
    "def blur_image( image, bboxes):\n",
    "    int_bboxes = bboxes.astype('int')\n",
    "    print(int_bboxes)\n",
    "    print(\"----------------\")\n",
    "    image = image.copy()\n",
    "    for box in int_bboxes:\n",
    "        cropped_img = image[box[1]:box[3], box[0]:box[2]]\n",
    "        blurred_img = cv2.GaussianBlur(cropped_img, (25,25),0)\n",
    "        x_offset, y_offset = box[0], box[1]\n",
    "        x_end = x_offset + blurred_img.shape[1]\n",
    "        y_end = y_offset + blurred_img.shape[0]\n",
    "        image[y_offset:y_end, x_offset:x_end] = blurred_img\n",
    "    return image\n",
    "\n",
    "\n",
    "analyzed_img = rec.analyze(img)\n",
    "result = blur_image(img, analyzed_img.boxes)\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "# original = Image.fromarray(img, 'RGB')\n",
    "# result2 = Image.fromarray(result, 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rec.result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 14 cars, 3 traffic lights, 12.5ms\n",
      "Speed: 2.0ms preprocess, 12.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.engine.results import Boxes\n",
    "image = cv2.imread(\"example.jpg\")\n",
    "#car_boxes = model.predict(image, save = True)\n",
    "# Let the NumberPlateRecognition run on the car bounding box\n",
    "\n",
    "\n",
    "def analyze(image):\n",
    "    result = model(image)[0]\n",
    "    data: Boxes = result.boxes.cpu().numpy()\n",
    "    return (data.xyxy,data.conf,data.cls,result.names)\n",
    "\n",
    "xyxy,conf,cls,names = analyze(image)\n",
    "print(names)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NumberPlateRecognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
