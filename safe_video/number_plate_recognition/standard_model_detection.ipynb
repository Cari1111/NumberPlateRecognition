{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from torchvision.models import detection\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", type=str, default=\"example2.jpg\",\n",
    "                # required=True,\n",
    "\thelp=\"path to the input image\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"frcnn-resnet\",\n",
    "\tchoices=[\"frcnn-resnet\", \"frcnn-mobilenet\", \"retinanet\"],\n",
    "\thelp=\"name of the object detection model\")\n",
    "ap.add_argument(\"-l\", \"--labels\", type=str, default=\"coco_classes.pickle\",\n",
    "\thelp=\"path to file containing list of categories in COCO dataset\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "\thelp=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_known_args()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device we will be using to run the model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# load the list of categories in the COCO dataset and then generate a\n",
    "# set of bounding box colors for each class\n",
    "CLASSES = pickle.loads(open(args[\"labels\"], \"rb\").read())\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary containing model name and its corresponding\n",
    "# torchvision function call\n",
    "MODELS = {\n",
    "\t\"frcnn-resnet\": detection.fasterrcnn_resnet50_fpn,\n",
    "\t\"frcnn-mobilenet\": detection.fasterrcnn_mobilenet_v3_large_320_fpn,\n",
    "\t\"retinanet\": detection.retinanet_resnet50_fpn\n",
    "}\n",
    "# load the model and set it to evaluation mode\n",
    "model = MODELS[args[\"model\"]](weights=True, progress=True,\n",
    "\tnum_classes=len(CLASSES), pretrained_backbone=True).to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image from disk\n",
    "image = cv2.imread(args[\"image\"])\n",
    "orig = image.copy()\n",
    "# convert the image from BGR to RGB channel ordering and change the\n",
    "# image from channels last to channels first ordering\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = image.transpose((2, 0, 1))\n",
    "# add the batch dimension, scale the raw pixel intensities to the\n",
    "# range [0, 1], and convert the image to a floating point tensor\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = image / 255.0\n",
    "image = torch.FloatTensor(image)\n",
    "# send the input to the device and pass the it through the network to\n",
    "# get the detections and predictions\n",
    "image = image.to(DEVICE)\n",
    "detections = model(image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the detections\n",
    "for i in range(0, len(detections[\"boxes\"])):\n",
    "\t# extract the confidence (i.e., probability) associated with the\n",
    "\t# prediction\n",
    "\tconfidence = detections[\"scores\"][i]\n",
    "\t# filter out weak detections by ensuring the confidence is\n",
    "\t# greater than the minimum confidence\n",
    "\tif confidence > args[\"confidence\"]:\n",
    "\t\t# extract the index of the class label from the detections,\n",
    "\t\t# then compute the (x, y)-coordinates of the bounding box\n",
    "\t\t# for the object\n",
    "\t\tidx = int(detections[\"labels\"][i])\n",
    "\t\tbox = detections[\"boxes\"][i].detach().cpu().numpy()\n",
    "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\t\t# display the prediction to our terminal\n",
    "\t\tlabel = \"{}: {:.2f}%\".format(CLASSES[idx], confidence * 100)\n",
    "\t\tprint(\"[INFO] {}\".format(label))\n",
    "\t\t# draw the bounding box and label on the image\n",
    "\t\tcv2.rectangle(orig, (startX, startY), (endX, endY),\n",
    "\t\t\tCOLORS[idx], 2)\n",
    "\t\ty = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "\t\tcv2.putText(orig, label, (startX, y),\n",
    "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "# show the output image\n",
    "# cv2.imshow(\"Output\", orig)\n",
    "# cv2.waitKey(0)\n",
    "from matplotlib import pyplot as plt\n",
    "cor = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(cor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NumberPlateRecognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
